{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vg6-4wPAYrK_",
        "outputId": "8aca2c48-e9d9-4b7d-8854-fcd495fcb8af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.11/dist-packages (2.1.3)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.17.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.11/dist-packages (from xgboost) (2.21.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from xgboost) (1.13.1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.1.21)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.12.1)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.25.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.70.0)\n",
            "Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.17.1)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.5.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.55.6)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.2.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.2.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.2.0->tensorflow) (0.14.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.12.14)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.2.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.2.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install xgboost tensorflow pandas numpy scikit-learn matplotlib\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import xgboost as xgb\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv(\"/content/complete_solar_data2.csv\", parse_dates=[\"DATE_TIME\"])\n",
        "df.sort_values(\"DATE_TIME\", inplace=True)\n",
        "df.drop_duplicates(inplace=True)\n",
        "\n",
        "# Feature Engineering\n",
        "df[\"Hour\"] = df[\"DATE_TIME\"].dt.hour\n",
        "df[\"Day\"] = df[\"DATE_TIME\"].dt.day\n",
        "df[\"Month\"] = df[\"DATE_TIME\"].dt.month\n",
        "df[\"Year\"] = df[\"DATE_TIME\"].dt.year\n",
        "\n",
        "# Drop original timestamp\n",
        "df.drop(columns=[\"DATE_TIME\"], inplace=True)\n",
        "\n",
        "# Splitting features and target\n",
        "target = \"AC_POWER\"\n",
        "features = df.drop(columns=[target])\n",
        "y = df[target]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    features, y, test_size=0.2, random_state=42, shuffle=False\n",
        ")\n",
        "\n",
        "# Train XGBoost Model\n",
        "xgb_model = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=100, learning_rate=0.1)\n",
        "xgb_model.fit(X_train, y_train)\n",
        "\n",
        "# Use XGBoost Predictions as Features\n",
        "xgb_train_pred = xgb_model.predict(X_train).reshape(-1, 1)\n",
        "xgb_test_pred = xgb_model.predict(X_test).reshape(-1, 1)\n",
        "\n",
        "# Normalize Data\n",
        "scaler = MinMaxScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Append XGBoost predictions to scaled data\n",
        "X_train_scaled = np.hstack((X_train_scaled, xgb_train_pred))\n",
        "X_test_scaled = np.hstack((X_test_scaled, xgb_test_pred))\n",
        "\n",
        "# Ensure the dataset is large enough for sequences\n",
        "time_steps = 10\n",
        "if len(X_train_scaled) <= time_steps or len(X_test_scaled) <= time_steps:\n",
        "    raise ValueError(\"Dataset too small for LSTM sequence creation!\")\n",
        "\n",
        "# Function to create LSTM sequences\n",
        "def create_sequences(X, y, time_steps=10):\n",
        "    Xs, ys = [], []\n",
        "    for i in range(len(X) - time_steps):\n",
        "        Xs.append(X[i : i + time_steps])\n",
        "        ys.append(y[i + time_steps])\n",
        "    return np.array(Xs), np.array(ys)\n",
        "\n",
        "# Create sequences\n",
        "X_train_seq, y_train_seq = create_sequences(X_train_scaled, y_train.to_numpy(), time_steps)\n",
        "X_test_seq, y_test_seq = create_sequences(X_test_scaled, y_test.to_numpy(), time_steps)\n",
        "\n",
        "# Build LSTM Model\n",
        "lstm_model = Sequential([\n",
        "    LSTM(64, return_sequences=True, input_shape=(X_train_seq.shape[1], X_train_seq.shape[2])),\n",
        "    Dropout(0.2),\n",
        "    LSTM(32, return_sequences=False),\n",
        "    Dropout(0.2),\n",
        "    Dense(1)\n",
        "])\n",
        "\n",
        "lstm_model.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "# Train LSTM Model\n",
        "lstm_model.fit(X_train_seq, y_train_seq, epochs=20, batch_size=32, validation_data=(X_test_seq, y_test_seq))\n",
        "\n",
        "# Predict with LSTM\n",
        "y_pred_lstm = lstm_model.predict(X_test_seq)\n",
        "\n",
        "# Ensure equal lengths before averaging\n",
        "final_pred = (y_pred_lstm.flatten() + xgb_test_pred[time_steps:].flatten()) / 2\n",
        "\n",
        "# Print first 10 predictions for verification\n",
        "print(\"Final Hybrid Model Predictions:\", final_pred[:10])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I7QtecdNZUoK",
        "outputId": "70976fc7-73e1-42f9-e5a6-c4043fa74926"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m1430/1430\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 6ms/step - loss: 236727.1406 - val_loss: 116740.6953\n",
            "Epoch 2/20\n",
            "\u001b[1m1430/1430\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - loss: 209614.2656 - val_loss: 100673.4531\n",
            "Epoch 3/20\n",
            "\u001b[1m1430/1430\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - loss: 187288.7812 - val_loss: 86757.0547\n",
            "Epoch 4/20\n",
            "\u001b[1m1430/1430\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - loss: 168723.7031 - val_loss: 74735.3906\n",
            "Epoch 5/20\n",
            "\u001b[1m1430/1430\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - loss: 150094.5156 - val_loss: 64328.5781\n",
            "Epoch 6/20\n",
            "\u001b[1m1430/1430\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 6ms/step - loss: 135669.6094 - val_loss: 55599.9609\n",
            "Epoch 7/20\n",
            "\u001b[1m1430/1430\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - loss: 122087.6094 - val_loss: 48422.3398\n",
            "Epoch 8/20\n",
            "\u001b[1m1430/1430\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - loss: 110621.7500 - val_loss: 42153.6445\n",
            "Epoch 9/20\n",
            "\u001b[1m1430/1430\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 6ms/step - loss: 98605.8516 - val_loss: 37243.8086\n",
            "Epoch 10/20\n",
            "\u001b[1m1430/1430\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - loss: 91136.8906 - val_loss: 33393.6484\n",
            "Epoch 11/20\n",
            "\u001b[1m1430/1430\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 6ms/step - loss: 84735.4766 - val_loss: 30533.8418\n",
            "Epoch 12/20\n",
            "\u001b[1m1430/1430\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 6ms/step - loss: 76922.1875 - val_loss: 28672.6914\n",
            "Epoch 13/20\n",
            "\u001b[1m1430/1430\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - loss: 72664.4297 - val_loss: 28014.6895\n",
            "Epoch 14/20\n",
            "\u001b[1m1430/1430\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - loss: 69320.3828 - val_loss: 26399.9727\n",
            "Epoch 15/20\n",
            "\u001b[1m1430/1430\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - loss: 65552.3594 - val_loss: 26112.6445\n",
            "Epoch 16/20\n",
            "\u001b[1m1430/1430\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - loss: 63013.1953 - val_loss: 25784.5703\n",
            "Epoch 17/20\n",
            "\u001b[1m1430/1430\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 6ms/step - loss: 60920.8594 - val_loss: 26578.6172\n",
            "Epoch 18/20\n",
            "\u001b[1m1430/1430\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - loss: 60431.1680 - val_loss: 26243.5762\n",
            "Epoch 19/20\n",
            "\u001b[1m1430/1430\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - loss: 60016.3242 - val_loss: 26597.9531\n",
            "Epoch 20/20\n",
            "\u001b[1m1430/1430\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - loss: 58654.8906 - val_loss: 27167.8066\n",
            "\u001b[1m358/358\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
            "Final Hybrid Model Predictions: [0.42247912 0.184216   0.22857043 0.72912407 0.5002513  0.2795758\n",
            " 0.17370841 0.04988334 0.16403148 0.0920243 ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv('/content/complete_solar_data2.csv')\n",
        "\n",
        "# Convert DATE_TIME to datetime format\n",
        "df['DATE_TIME'] = pd.to_datetime(df['DATE_TIME'], errors='coerce')\n",
        "\n",
        "# Extract useful time-based features\n",
        "df['YEAR'] = df['DATE_TIME'].dt.year\n",
        "df['MONTH'] = df['DATE_TIME'].dt.month\n",
        "df['DAY'] = df['DATE_TIME'].dt.day\n",
        "df['HOUR'] = df['DATE_TIME'].dt.hour\n",
        "\n",
        "# Drop DATE_TIME column\n",
        "df.drop(columns=['DATE_TIME'], inplace=True)\n",
        "\n",
        "# Drop NaN values (if any)\n",
        "df.dropna(inplace=True)\n",
        "\n",
        "# Define target column\n",
        "target_column = 'AC_POWER'\n",
        "\n",
        "# Separate features and target\n",
        "features = df.drop(columns=[target_column])\n",
        "target = df[target_column]\n",
        "\n",
        "# Feature Scaling\n",
        "scaler = StandardScaler()\n",
        "features_scaled = scaler.fit_transform(features)\n",
        "\n",
        "# Splitting Data\n",
        "X_train, X_test, y_train, y_test = train_test_split(features_scaled, target, test_size=0.2, random_state=42)\n",
        "\n",
        "# Reshape data for LSTM\n",
        "X_train = np.reshape(X_train, (X_train.shape[0], 1, X_train.shape[1]))\n",
        "X_test = np.reshape(X_test, (X_test.shape[0], 1, X_test.shape[1]))\n",
        "\n",
        "# Build LSTM Model\n",
        "model = Sequential([\n",
        "    LSTM(64, return_sequences=True, input_shape=(1, X_train.shape[2])),\n",
        "    Dropout(0.2),\n",
        "    LSTM(32, return_sequences=False),\n",
        "    Dropout(0.2),\n",
        "    Dense(16, activation='relu'),\n",
        "    Dense(1, activation='relu')  # Ensure correct activation for regression\n",
        "])\n",
        "\n",
        "# Compile Model\n",
        "model.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "# Callbacks\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, verbose=1)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6, verbose=1)\n",
        "\n",
        "# Train Model\n",
        "history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test),\n",
        "                    callbacks=[early_stopping, reduce_lr])\n",
        "\n",
        "# Predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Performance Metrics\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "rmse = np.sqrt(mse)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "# Calculate Accuracy using MAPE\n",
        "mape = np.mean(np.abs((y_test - y_pred.flatten()) / y_test)) * 100\n",
        "accuracy = 100 - mape  # Accuracy formula\n",
        "\n",
        "# Print Results\n",
        "print(f\"Mean Absolute Error (MAE): {mae}\")\n",
        "print(f\"Mean Squared Error (MSE): {mse}\")\n",
        "print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n",
        "print(f\"R² Score: {r2}\")\n",
        "print(f\"Model Accuracy (100 - MAPE): {accuracy:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1aQv2iQhfLTt",
        "outputId": "80ee0a0e-6669-4b67-86da-8e6fb08c6f4a"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1693/1693\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 6ms/step - loss: 148527.4219 - val_loss: 47584.8750 - learning_rate: 0.0010\n",
            "Epoch 2/50\n",
            "\u001b[1m1693/1693\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 6ms/step - loss: 38748.3555 - val_loss: 16965.0449 - learning_rate: 0.0010\n",
            "Epoch 3/50\n",
            "\u001b[1m1693/1693\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - loss: 19946.0781 - val_loss: 15352.1348 - learning_rate: 0.0010\n",
            "Epoch 4/50\n",
            "\u001b[1m1693/1693\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - loss: 18545.8086 - val_loss: 14821.6855 - learning_rate: 0.0010\n",
            "Epoch 5/50\n",
            "\u001b[1m1693/1693\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - loss: 18632.6621 - val_loss: 14433.4170 - learning_rate: 0.0010\n",
            "Epoch 6/50\n",
            "\u001b[1m1693/1693\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - loss: 17548.2969 - val_loss: 14263.7207 - learning_rate: 0.0010\n",
            "Epoch 7/50\n",
            "\u001b[1m1693/1693\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - loss: 17026.4785 - val_loss: 14142.7812 - learning_rate: 0.0010\n",
            "Epoch 8/50\n",
            "\u001b[1m1693/1693\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - loss: 17603.3457 - val_loss: 13982.8135 - learning_rate: 0.0010\n",
            "Epoch 9/50\n",
            "\u001b[1m1693/1693\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - loss: 17199.7129 - val_loss: 13888.3711 - learning_rate: 0.0010\n",
            "Epoch 10/50\n",
            "\u001b[1m1693/1693\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - loss: 16949.8633 - val_loss: 13797.8164 - learning_rate: 0.0010\n",
            "Epoch 11/50\n",
            "\u001b[1m1693/1693\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 6ms/step - loss: 17496.0840 - val_loss: 13988.7871 - learning_rate: 0.0010\n",
            "Epoch 12/50\n",
            "\u001b[1m1693/1693\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - loss: 16843.4258 - val_loss: 13535.7334 - learning_rate: 0.0010\n",
            "Epoch 13/50\n",
            "\u001b[1m1693/1693\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - loss: 16786.8809 - val_loss: 13508.6455 - learning_rate: 0.0010\n",
            "Epoch 14/50\n",
            "\u001b[1m1693/1693\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - loss: 15987.2979 - val_loss: 14029.0977 - learning_rate: 0.0010\n",
            "Epoch 15/50\n",
            "\u001b[1m1693/1693\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 6ms/step - loss: 16607.2891 - val_loss: 13720.4111 - learning_rate: 0.0010\n",
            "Epoch 16/50\n",
            "\u001b[1m1689/1693\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 16270.7393\n",
            "Epoch 16: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "\u001b[1m1693/1693\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - loss: 16271.0410 - val_loss: 13699.2021 - learning_rate: 0.0010\n",
            "Epoch 17/50\n",
            "\u001b[1m1693/1693\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 5ms/step - loss: 15902.3848 - val_loss: 13377.5732 - learning_rate: 5.0000e-04\n",
            "Epoch 18/50\n",
            "\u001b[1m1693/1693\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - loss: 15865.7607 - val_loss: 13379.7559 - learning_rate: 5.0000e-04\n",
            "Epoch 19/50\n",
            "\u001b[1m1693/1693\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - loss: 16571.2422 - val_loss: 13342.1650 - learning_rate: 5.0000e-04\n",
            "Epoch 20/50\n",
            "\u001b[1m1693/1693\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 6ms/step - loss: 16283.4590 - val_loss: 13359.1377 - learning_rate: 5.0000e-04\n",
            "Epoch 21/50\n",
            "\u001b[1m1693/1693\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - loss: 15894.2705 - val_loss: 13373.6436 - learning_rate: 5.0000e-04\n",
            "Epoch 22/50\n",
            "\u001b[1m1693/1693\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - loss: 15827.9053 - val_loss: 13223.6631 - learning_rate: 5.0000e-04\n",
            "Epoch 23/50\n",
            "\u001b[1m1693/1693\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - loss: 15551.5859 - val_loss: 13240.7334 - learning_rate: 5.0000e-04\n",
            "Epoch 24/50\n",
            "\u001b[1m1693/1693\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - loss: 15500.6436 - val_loss: 13295.9258 - learning_rate: 5.0000e-04\n",
            "Epoch 25/50\n",
            "\u001b[1m1692/1693\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 15434.3467\n",
            "Epoch 25: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "\u001b[1m1693/1693\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - loss: 15434.7520 - val_loss: 13348.7461 - learning_rate: 5.0000e-04\n",
            "Epoch 26/50\n",
            "\u001b[1m1693/1693\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - loss: 15917.8750 - val_loss: 13199.7920 - learning_rate: 2.5000e-04\n",
            "Epoch 27/50\n",
            "\u001b[1m1693/1693\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 6ms/step - loss: 15947.9316 - val_loss: 13213.4189 - learning_rate: 2.5000e-04\n",
            "Epoch 28/50\n",
            "\u001b[1m1693/1693\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 6ms/step - loss: 15303.8613 - val_loss: 13183.6348 - learning_rate: 2.5000e-04\n",
            "Epoch 29/50\n",
            "\u001b[1m1693/1693\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - loss: 15722.0771 - val_loss: 13386.1611 - learning_rate: 2.5000e-04\n",
            "Epoch 30/50\n",
            "\u001b[1m1693/1693\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 6ms/step - loss: 15590.7295 - val_loss: 13269.7949 - learning_rate: 2.5000e-04\n",
            "Epoch 31/50\n",
            "\u001b[1m1684/1693\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 15508.4854\n",
            "Epoch 31: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "\u001b[1m1693/1693\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - loss: 15509.4238 - val_loss: 13187.8818 - learning_rate: 2.5000e-04\n",
            "Epoch 32/50\n",
            "\u001b[1m1693/1693\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - loss: 15465.2656 - val_loss: 13158.4941 - learning_rate: 1.2500e-04\n",
            "Epoch 33/50\n",
            "\u001b[1m1693/1693\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 6ms/step - loss: 15464.6641 - val_loss: 13142.2861 - learning_rate: 1.2500e-04\n",
            "Epoch 34/50\n",
            "\u001b[1m1693/1693\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - loss: 15343.1562 - val_loss: 13157.1816 - learning_rate: 1.2500e-04\n",
            "Epoch 35/50\n",
            "\u001b[1m1693/1693\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - loss: 15261.2754 - val_loss: 13155.8574 - learning_rate: 1.2500e-04\n",
            "Epoch 36/50\n",
            "\u001b[1m1690/1693\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 15063.4482\n",
            "Epoch 36: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
            "\u001b[1m1693/1693\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - loss: 15064.4141 - val_loss: 13221.8740 - learning_rate: 1.2500e-04\n",
            "Epoch 37/50\n",
            "\u001b[1m1693/1693\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 6ms/step - loss: 15485.6055 - val_loss: 13167.2080 - learning_rate: 6.2500e-05\n",
            "Epoch 38/50\n",
            "\u001b[1m1693/1693\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - loss: 14971.4023 - val_loss: 13129.9844 - learning_rate: 6.2500e-05\n",
            "Epoch 39/50\n",
            "\u001b[1m1693/1693\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - loss: 15026.1533 - val_loss: 13133.4795 - learning_rate: 6.2500e-05\n",
            "Epoch 40/50\n",
            "\u001b[1m1693/1693\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - loss: 15686.9805 - val_loss: 13138.7871 - learning_rate: 6.2500e-05\n",
            "Epoch 41/50\n",
            "\u001b[1m1686/1693\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 15204.1387\n",
            "Epoch 41: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
            "\u001b[1m1693/1693\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 6ms/step - loss: 15204.7275 - val_loss: 13137.7656 - learning_rate: 6.2500e-05\n",
            "Epoch 42/50\n",
            "\u001b[1m1693/1693\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - loss: 15431.8379 - val_loss: 13125.7510 - learning_rate: 3.1250e-05\n",
            "Epoch 43/50\n",
            "\u001b[1m1693/1693\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - loss: 15019.9609 - val_loss: 13113.7656 - learning_rate: 3.1250e-05\n",
            "Epoch 44/50\n",
            "\u001b[1m1693/1693\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - loss: 15784.3057 - val_loss: 13120.2354 - learning_rate: 3.1250e-05\n",
            "Epoch 45/50\n",
            "\u001b[1m1693/1693\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - loss: 15567.5508 - val_loss: 13122.1553 - learning_rate: 3.1250e-05\n",
            "Epoch 46/50\n",
            "\u001b[1m1689/1693\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 15473.1191\n",
            "Epoch 46: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
            "\u001b[1m1693/1693\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - loss: 15473.0234 - val_loss: 13117.4463 - learning_rate: 3.1250e-05\n",
            "Epoch 47/50\n",
            "\u001b[1m1693/1693\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 6ms/step - loss: 15130.9600 - val_loss: 13120.1367 - learning_rate: 1.5625e-05\n",
            "Epoch 48/50\n",
            "\u001b[1m1693/1693\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 6ms/step - loss: 15700.7305 - val_loss: 13116.8867 - learning_rate: 1.5625e-05\n",
            "Epoch 48: early stopping\n",
            "Restoring model weights from the end of the best epoch: 43.\n",
            "\u001b[1m424/424\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
            "Mean Absolute Error (MAE): 37.47689266251725\n",
            "Mean Squared Error (MSE): 13113.762563860337\n",
            "Root Mean Squared Error (RMSE): 114.51533767954551\n",
            "R² Score: 0.8993833925710912\n",
            "Model Accuracy (100 - MAPE): -inf%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional, BatchNormalization\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv('/content/complete_solar_data2.csv')\n",
        "\n",
        "# Convert DATE_TIME to datetime format\n",
        "df['DATE_TIME'] = pd.to_datetime(df['DATE_TIME'], errors='coerce')\n",
        "\n",
        "# Extract time-based features\n",
        "df['YEAR'] = df['DATE_TIME'].dt.year\n",
        "df['MONTH'] = df['DATE_TIME'].dt.month\n",
        "df['DAY'] = df['DATE_TIME'].dt.day\n",
        "df['HOUR'] = df['DATE_TIME'].dt.hour\n",
        "\n",
        "# Drop DATE_TIME column\n",
        "df.drop(columns=['DATE_TIME'], inplace=True)\n",
        "\n",
        "# Drop NaN values (if any)\n",
        "df.dropna(inplace=True)\n",
        "\n",
        "# Define target column\n",
        "target_column = 'AC_POWER'\n",
        "\n",
        "# Separate features and target\n",
        "features = df.drop(columns=[target_column])\n",
        "target = df[target_column]\n",
        "\n",
        "# Feature Scaling\n",
        "scaler = StandardScaler()\n",
        "features_scaled = scaler.fit_transform(features)\n",
        "\n",
        "# Splitting Data\n",
        "X_train, X_test, y_train, y_test = train_test_split(features_scaled, target, test_size=0.2, random_state=42)\n",
        "\n",
        "# Reshape data for LSTM (Sliding Window Technique)\n",
        "X_train = np.reshape(X_train, (X_train.shape[0], 1, X_train.shape[1]))\n",
        "X_test = np.reshape(X_test, (X_test.shape[0], 1, X_test.shape[1]))\n",
        "\n",
        "# Build Improved LSTM Model\n",
        "model = Sequential([\n",
        "    Bidirectional(LSTM(128, return_sequences=True, input_shape=(1, X_train.shape[2]))),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.3),\n",
        "\n",
        "    Bidirectional(LSTM(64, return_sequences=True)),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.3),\n",
        "\n",
        "    LSTM(32, return_sequences=False),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.3),\n",
        "\n",
        "    Dense(32, activation='relu'),\n",
        "    Dense(16, activation='relu'),\n",
        "    Dense(1, activation='relu')  # Output layer\n",
        "])\n",
        "\n",
        "# Compile Model\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss='mse')\n",
        "\n",
        "# Callbacks\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=7, restore_best_weights=True, verbose=1)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6, verbose=1)\n",
        "\n",
        "# Train Model\n",
        "history = model.fit(X_train, y_train, epochs=100, batch_size=64, validation_data=(X_test, y_test),\n",
        "                    callbacks=[early_stopping, reduce_lr])\n",
        "\n",
        "# Predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Performance Metrics\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "rmse = np.sqrt(mse)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "# Calculate Accuracy using MAPE\n",
        "mape = np.mean(np.abs((y_test - y_pred.flatten()) / y_test)) * 100\n",
        "accuracy = 100 - mape  # Accuracy formula\n",
        "\n",
        "# Print Results\n",
        "print(f\"Mean Absolute Error (MAE): {mae}\")\n",
        "print(f\"Mean Squared Error (MSE): {mse}\")\n",
        "print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n",
        "print(f\"R² Score: {r2}\")\n",
        "print(f\"Model Accuracy (100 - MAPE): {accuracy:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_GaAlGsmjXOZ",
        "outputId": "7faa52c7-0666-42ea-9703-5eb42500f1ce"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 13ms/step - loss: 92990.0234 - val_loss: 17010.2051 - learning_rate: 0.0010\n",
            "Epoch 2/100\n",
            "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 13ms/step - loss: 22931.0254 - val_loss: 15978.2207 - learning_rate: 0.0010\n",
            "Epoch 3/100\n",
            "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 13ms/step - loss: 21604.7930 - val_loss: 14770.6201 - learning_rate: 0.0010\n",
            "Epoch 4/100\n",
            "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 13ms/step - loss: 20876.9844 - val_loss: 13670.2939 - learning_rate: 0.0010\n",
            "Epoch 5/100\n",
            "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 13ms/step - loss: 20015.1016 - val_loss: 14858.7773 - learning_rate: 0.0010\n",
            "Epoch 6/100\n",
            "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 13ms/step - loss: 19900.0840 - val_loss: 14583.3809 - learning_rate: 0.0010\n",
            "Epoch 7/100\n",
            "\u001b[1m846/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 19640.8340\n",
            "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 13ms/step - loss: 19639.5078 - val_loss: 13820.9404 - learning_rate: 0.0010\n",
            "Epoch 8/100\n",
            "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 12ms/step - loss: 17491.9824 - val_loss: 13774.1250 - learning_rate: 5.0000e-04\n",
            "Epoch 9/100\n",
            "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 13ms/step - loss: 17709.0332 - val_loss: 12994.6162 - learning_rate: 5.0000e-04\n",
            "Epoch 10/100\n",
            "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 13ms/step - loss: 18078.2441 - val_loss: 13039.4453 - learning_rate: 5.0000e-04\n",
            "Epoch 11/100\n",
            "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 13ms/step - loss: 16976.0703 - val_loss: 12781.3965 - learning_rate: 5.0000e-04\n",
            "Epoch 12/100\n",
            "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 13ms/step - loss: 17402.7402 - val_loss: 12975.8350 - learning_rate: 5.0000e-04\n",
            "Epoch 13/100\n",
            "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 12ms/step - loss: 16873.0625 - val_loss: 12963.0225 - learning_rate: 5.0000e-04\n",
            "Epoch 14/100\n",
            "\u001b[1m846/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 16784.9551\n",
            "Epoch 14: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 13ms/step - loss: 16785.1758 - val_loss: 12985.0693 - learning_rate: 5.0000e-04\n",
            "Epoch 15/100\n",
            "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 13ms/step - loss: 16056.3604 - val_loss: 12476.8760 - learning_rate: 2.5000e-04\n",
            "Epoch 16/100\n",
            "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 13ms/step - loss: 16348.4775 - val_loss: 12820.0654 - learning_rate: 2.5000e-04\n",
            "Epoch 17/100\n",
            "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 13ms/step - loss: 15845.6133 - val_loss: 12993.3203 - learning_rate: 2.5000e-04\n",
            "Epoch 18/100\n",
            "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 16237.2002\n",
            "Epoch 18: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 13ms/step - loss: 16237.1396 - val_loss: 12719.0537 - learning_rate: 2.5000e-04\n",
            "Epoch 19/100\n",
            "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 13ms/step - loss: 15838.2656 - val_loss: 12532.1992 - learning_rate: 1.2500e-04\n",
            "Epoch 20/100\n",
            "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 13ms/step - loss: 16407.8516 - val_loss: 12301.2588 - learning_rate: 1.2500e-04\n",
            "Epoch 21/100\n",
            "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 13ms/step - loss: 15578.0742 - val_loss: 12348.0410 - learning_rate: 1.2500e-04\n",
            "Epoch 22/100\n",
            "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 13ms/step - loss: 16247.0186 - val_loss: 12146.4287 - learning_rate: 1.2500e-04\n",
            "Epoch 23/100\n",
            "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 13ms/step - loss: 16228.0371 - val_loss: 12390.1025 - learning_rate: 1.2500e-04\n",
            "Epoch 24/100\n",
            "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 13ms/step - loss: 16131.9160 - val_loss: 12271.1514 - learning_rate: 1.2500e-04\n",
            "Epoch 25/100\n",
            "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 15520.4023\n",
            "Epoch 25: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
            "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 13ms/step - loss: 15520.3330 - val_loss: 12203.5654 - learning_rate: 1.2500e-04\n",
            "Epoch 26/100\n",
            "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 12ms/step - loss: 15675.7070 - val_loss: 11978.5078 - learning_rate: 6.2500e-05\n",
            "Epoch 27/100\n",
            "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 13ms/step - loss: 15697.8779 - val_loss: 12070.9355 - learning_rate: 6.2500e-05\n",
            "Epoch 28/100\n",
            "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 13ms/step - loss: 15213.7344 - val_loss: 12060.3047 - learning_rate: 6.2500e-05\n",
            "Epoch 29/100\n",
            "\u001b[1m846/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 15766.3301\n",
            "Epoch 29: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
            "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 13ms/step - loss: 15765.4346 - val_loss: 12033.5654 - learning_rate: 6.2500e-05\n",
            "Epoch 30/100\n",
            "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 13ms/step - loss: 15623.0596 - val_loss: 12001.3213 - learning_rate: 3.1250e-05\n",
            "Epoch 31/100\n",
            "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 13ms/step - loss: 15630.4277 - val_loss: 12010.1484 - learning_rate: 3.1250e-05\n",
            "Epoch 32/100\n",
            "\u001b[1m844/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 14994.0000\n",
            "Epoch 32: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
            "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 13ms/step - loss: 14995.1562 - val_loss: 11988.0967 - learning_rate: 3.1250e-05\n",
            "Epoch 33/100\n",
            "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 13ms/step - loss: 14992.8467 - val_loss: 12109.6016 - learning_rate: 1.5625e-05\n",
            "Epoch 33: early stopping\n",
            "Restoring model weights from the end of the best epoch: 26.\n",
            "\u001b[1m424/424\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step\n",
            "Mean Absolute Error (MAE): 38.51143874886403\n",
            "Mean Squared Error (MSE): 11978.506595871993\n",
            "Root Mean Squared Error (RMSE): 109.44636401394061\n",
            "R² Score: 0.9080937534233762\n",
            "Model Accuracy (100 - MAPE): -inf%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to take user input and make a prediction\n",
        "def predict_ac_power():\n",
        "    print(\"\\nEnter feature values for prediction:\")\n",
        "\n",
        "    # Accept user input for all features\n",
        "    feature_values = []\n",
        "    for col in features.columns:\n",
        "        val = float(input(f\"Enter {col}: \"))\n",
        "        feature_values.append(val)\n",
        "\n",
        "    # Convert input to numpy array\n",
        "    feature_array = np.array(feature_values).reshape(1, -1)\n",
        "\n",
        "    # Scale input using the previously fitted scaler\n",
        "    feature_array_scaled = scaler.transform(feature_array)\n",
        "\n",
        "    # Reshape for LSTM input\n",
        "    feature_array_reshaped = np.reshape(feature_array_scaled, (1, 1, feature_array_scaled.shape[1]))\n",
        "\n",
        "    # Make prediction\n",
        "    predicted_power = model.predict(feature_array_reshaped)\n",
        "\n",
        "    # Print the prediction\n",
        "    print(f\"\\nPredicted AC Power: {predicted_power[0][0]:.2f}\")\n",
        "\n",
        "# Call function for user input and prediction\n",
        "predict_ac_power()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7tY7xWz2qwZW",
        "outputId": "afa225f9-47bc-4677-a042-ac8beefaadd4"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Enter feature values for prediction:\n",
            "Enter AMBIENT_TEMPERATURE: 60\n",
            "Enter MODULE_TEMPERATURE: 3\n",
            "Enter IRRADIATION: 12\n",
            "Enter DAILY_YIELD: 23\n",
            "Enter YEAR: 2022\n",
            "Enter MONTH: 3\n",
            "Enter DAY: 6\n",
            "Enter HOUR: 23\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\n",
            "Predicted AC Power: 1043.18\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    }
  ]
}